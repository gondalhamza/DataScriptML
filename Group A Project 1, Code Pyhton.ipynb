{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#store the url in a variable\n",
    "url = \"train_data.csv\"\n",
    "url_test = \"test_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in the data with `read_csv()`\n",
    "# #dataset = pd.read_csv(url)\n",
    "# test_dataset = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in white wine data \n",
    "white = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep=';')\n",
    "\n",
    "# Read in red wine data \n",
    "red = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Merge Red and White\n",
    "# Add `type` column to `red` with value 1\n",
    "red['type'] = 1\n",
    "\n",
    "# Add `type` column to `white` with value 0\n",
    "white['type'] = 2\n",
    "\n",
    "# Append `white` to `red`\n",
    "wines = red.append(white, ignore_index=True)\n",
    "dataset = wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .head() method to view the first few records of the data set\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using head() method with an argument which helps us to restrict the number of initial records that should be displayed\n",
    "dataset.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .tail() method to view the last few records from the dataframe\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the dtypes() method to display the different datatypes available\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['quality'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check correleation between the variables using Seaborn's pairplot. \n",
    "fig = plt.figure()\n",
    "sns.set()\n",
    "sns_plot = sns.pairplot(dataset)\n",
    "sns_plot.savefig(\"pairplot_correlation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = dataset.corr()\n",
    "# display(correlation)\n",
    "plt.figure(figsize=(14, 12))\n",
    "heatmap = sns.heatmap(correlation, annot=True, linewidths=0, vmin=-1, cmap=\"RdBu_r\")\n",
    "heatmap.figure.savefig(\"heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the seaborn module\n",
    "#import seaborn as sns\n",
    "\n",
    "# import the matplotlib module\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set the background colour of the plot to white\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "# setting the plot size for all plots\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "# create a countplot \n",
    "sns_plot = sns.countplot('type',data=dataset,hue = 'quality')\n",
    "\n",
    "# Remove the top and down margin\n",
    "sns.despine(offset=10, trim=True)\n",
    "\n",
    "sns_plot.figure.savefig(\"quality_coun_plot.png\")\n",
    "# display the plotplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the seaborn module\n",
    "#import seaborn as sns\n",
    "\n",
    "# import the matplotlib module\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setting the plot size for all plots\n",
    "sns.set(rc={'figure.figsize':(16.7,13.27)})\n",
    "dataset.tail(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the violinplot\n",
    "sns_plot = sns.violinplot(x=\"quality\",y=\"fixed acidity\", hue=\"quality\", data=dataset);\n",
    "plt.show()\n",
    "sns_plot.figure.savefig(\"fixed_acidity_quality_violinplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of each target variable\n",
    "from collections import Counter\n",
    "Counter(dataset['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of the target variable\n",
    "sns_plot = sns.countplot(x='quality', data=dataset)\n",
    "sns_plot.figure.savefig(\"qualty count bothtypes countplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a boxplot to check for Outliers\n",
    "#Target variable is Quality. So will plot a boxplot each column against target variable\n",
    "sns_plot = sns.boxplot('quality', 'fixed acidity', data = dataset)\n",
    "sns_plot.figure.savefig(\"fixed acidity quality boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot('quality', 'volatile acidity', data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot('quality', 'citric acid', data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns_plot = sns.boxplot('quality', 'residual sugar', data = dataset)\n",
    "sns_plot.figure.savefig(\"residual sugar quality boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot('quality', 'chlorides', data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot('quality', 'free sulfur dioxide', data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot('quality', 'total sulfur dioxide', data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.boxplot('quality', 'density', data = dataset)\n",
    "sns_plot.figure.savefig(\"density quality boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot('quality', 'pH', data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot('quality', 'sulphates', data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot('quality', 'alcohol', data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1 = dataset['sulphates'].quantile(0.1)\n",
    "Q3 = dataset['sulphates'].quantile(0.95)\n",
    "IQR = Q3 - Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = dataset.quantile(0.25)\n",
    "Q3 = dataset.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetout = dataset[~((dataset < (Q1 - 1.5 * IQR)) |(dataset > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "datasetout.shape\n",
    "dataset=datasetout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature find the data points with extreme high or low values\n",
    "data= dataset\n",
    "for feature in data.keys():\n",
    "\n",
    "    # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "    Q1 = np.percentile(data[feature], q=20)\n",
    " \n",
    "    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "    Q3 = np.percentile(data[feature], q=70)\n",
    " \n",
    "    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "    interquartile_range = Q3 - Q1\n",
    "    step = 1.5 * interquartile_range\n",
    " \n",
    "    # Display the outliers\n",
    "    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "    display(data[~((data[feature] >= Q1 - step) & (data[feature] <= Q3 + step))])\n",
    " \n",
    "# OPTIONAL: Select the indices for data points you wish to remove\n",
    "outliers = []\n",
    "# Remove the outliers, if any were specified\n",
    "good_data = data.drop(data.index[outliers]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.boxplot('quality', 'residual sugar', data = dataset)\n",
    "sns_plot.figure.savefig(\"residual sugar quality clean boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplots show many outliers for quite a few columns. Describe the dataset to get a better idea on what's happening\n",
    "dataset.describe()\n",
    "#fixed acidity - 25% - 7.1 and 50% - 7.9. Not much of a variance. Could explain the huge number of outliers\n",
    "#volatile acididty - similar reasoning\n",
    "#citric acid - seems to be somewhat uniformly distributed\n",
    "#residual sugar - min - 0.9, max - 15!! Waaaaay too much difference. Could explain the outliers.\n",
    "#chlorides - same as residual sugar. Min - 0.012, max - 0.611\n",
    "#free sulfur dioxide, total suflur dioxide - same explanation as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .tail() method to view the last few records from the dataframe\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"type' : \",dataset['type'].unique())\n",
    "print(\"fixed acidity : \",dataset['fixed acidity'].unique())\n",
    "print(\"alcohol : \",dataset['alcohol'].unique())\n",
    "print(\"pH : \",dataset['pH'].unique())\n",
    "print(\"free sulfur dioxide : \",dataset['free sulfur dioxide'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary module\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# create the Labelencoder object\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "#display the initial records\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the dtypes() method to display the different datatypes available\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set & Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns other than 'POST-OPERATION-RISK'\n",
    "\n",
    "#cols = [col for col in dataset.columns if col not in ['quality','citric acid','residual sugar','chlorides','density','pH','sulphates','alcohol','type']]\n",
    "cols = [col for col in dataset.columns if col not in ['quality']]\n",
    "#cols = [col for col in dataset.columns if col not in ['quality','residual sugar','density']]\n",
    "\n",
    "#Attaching Quality and Type Column into Target Variable\n",
    "cols_all_target = [col for col in dataset.columns if col not in ['fixed acidity','volatile acidity',\n",
    "                                                                 'free sulfur dioxide','total sulfur dioxide',\n",
    "                                                                 'citric acid','residual sugar','chlorides',\n",
    "                                                                 'density','pH','sulphates','alcohol']]\n",
    "\n",
    "# dropping the 'POST-OPERATION-RISK' columns\n",
    "data = dataset[cols]\n",
    "\n",
    "#assigning the POST-OPERATION-RISK column as target\n",
    "target = dataset['quality']\n",
    "\n",
    "target_all = dataset[cols_all_target]\n",
    "\n",
    "data.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = target_all\n",
    "target2 = target_all\n",
    "\n",
    "target1 = target1[(target1['type']==1)]\n",
    "target2 = target2[(target2['type']==2)]\n",
    "target1 = target1['quality']\n",
    "target2 = target2['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data\n",
    "data2 = data\n",
    "\n",
    "data1 = data1[(data1['type']==1)]\n",
    "data2 = data2[(data2['type']==2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data set into train and test sets \n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.25, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data set into train and test sets \n",
    "data_train1, data_test1, target_train1, target_test1 = train_test_split(red.drop(columns=['quality']),red[['type', 'quality']], test_size = 0.25, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data set into train and test sets \n",
    "data_train2, data_test2, target_train2, target_test2 = train_test_split(white.drop(columns=['quality']),white[['type', 'quality']], test_size = 0.25, random_state = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## Scores Without Scaling\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=100)\n",
    "clf.fit(data_train,target_train)\n",
    "pred=clf.predict(data_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random Forest Accuracy Score: \",accuracy_score(target_test,pred, normalize = True))\n",
    "#Checking the accuracy \n",
    "r = r2_score(target_test, pred)\n",
    "mae = mean_absolute_error(target_test,pred)\n",
    "print(\"------Results for Random Forest-----\")\n",
    "print(\"1.....R score:  \",r)\n",
    "print(\"2.....Mean Absolute Error: \",mae)\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (mae / target_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('......Accuracy:', round(accuracy, 2), '%.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(target_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCORES WITH SCALING\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import `StandardScaler` from `sklearn.preprocessing`\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(data_train)\n",
    "\n",
    "# Scale the train set\n",
    "X_train = scaler.fit_transform(data_train)\n",
    "\n",
    "# Scale the test set\n",
    "X_test = scaler.transform(data_test)\n",
    "\n",
    "\n",
    "#-------\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=100)\n",
    "clf.fit(X_train,target_train)\n",
    "pred=clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random Forest Accuracy Score: \",accuracy_score(target_test,pred, normalize = True))\n",
    "#Checking the accuracy \n",
    "mse = mean_squared_error(target_test, pred)\n",
    "r = r2_score(target_test, pred)\n",
    "mae = mean_absolute_error(target_test,pred)\n",
    "print(\"------Results for Random Forest-----\")\n",
    "print(\"1.....R score:  \",r)\n",
    "print(\"2.....Mean Absolute Error: \",mae)\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (mae / target_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('......Accuracy:', round(accuracy, 2), '%.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scaling Min MAx\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## Scores\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(data_train)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=100)\n",
    "clf.fit(X_train_scaled,target_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(data_test)\n",
    "\n",
    "pred=clf.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(\"Random Forest Accuracy Score: \",accuracy_score(target_test,pred, normalize = True))\n",
    "#Checking the accuracy \n",
    "r = r2_score(target_test, pred)\n",
    "mae = mean_absolute_error(target_test,pred)\n",
    "print(\"------Results for Random Forest-----\")\n",
    "print(\"1.....R score:  \",r)\n",
    "print(\"2.....Mean Absolute Error: \",mae)\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (mae / target_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('......Accuracy:', round(accuracy, 2), '%.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [1, 2, 4, 8, 16, 32,42, 64, 100, 200,300,400,470,1100,1150]\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "for estimator in n_estimators:\n",
    "\n",
    "   rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1,  max_features = 'sqrt')\n",
    "   rf.fit(data_train, target_train)\n",
    "   train_pred = rf.predict(data_test)\n",
    "   print(\"Random Forest: \",estimator,accuracy_score(target_test, train_pred, normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "clf = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3,\n",
    "                               verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "model=clf.fit(data_train, target_train)\n",
    "\n",
    "# print winning set of hyperparameters\n",
    "from pprint import pprint\n",
    "pprint(model.best_estimator_.get_params())\n",
    "\n",
    "print(\"Best Params--\",model.best_params_)\n",
    "\n",
    "pred_RS = model.predict(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics Accuracy--\",accuracy_score(target_test,pred_RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy',random_state = 200)\n",
    "classifier.fit(data_train, target_train)\n",
    "\n",
    "pred = classifier.predict(data_test)\n",
    "\n",
    "print(\"Accuracy is:\",accuracy_score(target_test, pred, normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance \n",
    "\n",
    "features = cols\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(data_test.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))\n",
    "    plt.figure(figsize=(20,10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(data_test.shape[1]), importances[indices], color=\"green\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(data_test.shape[1]), [features[i] for i in indices])\n",
    "plt.xlim([-1, data_test.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XgBoost\n",
    "# First XGBoost model dataset\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(data_train, target_train)\n",
    "print(model)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(data_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(target_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators= 1000, random_state=100)\n",
    "rf.fit(data_train1, target_train1)\n",
    "train_pred1 = rf.predict(data_test1)\n",
    "    \n",
    "rf.fit(data_train2, target_train2)\n",
    "train_pred2 = rf.predict(data_test2)\n",
    "   \n",
    "all_predc = np.concatenate((train_pred1, train_pred2))\n",
    "all_target_testc = np.concatenate((target_test1, target_test2)) \n",
    "\n",
    "# print(\"Random Forest type1 Accuracy Score: \",accuracy_score(target_test1, train_pred1, normalize = True))\n",
    "# #Checking the accuracy \n",
    "# mse = mean_squared_error(target_test1, train_pred1)\n",
    "# r = r2_score(target_test1, train_pred1)\n",
    "# mae = mean_absolute_error(target_test1,train_pred1)\n",
    "# print(\"------Results for Random Forest-----\")\n",
    "# print(\"1.....Mean Squared Error: \",mse)\n",
    "# print(\"2.....R score:  \",r)\n",
    "# print(\"3.....Mean Absolute Error: \",mae)\n",
    "\n",
    "\n",
    "# print(\"Random Forest type2: \",accuracy_score(target_test2, train_pred2, normalize = True))\n",
    "# #Checking the accuracy \n",
    "# mse = mean_squared_error(target_test2, train_pred2)\n",
    "# r = r2_score(target_test2, train_pred2)\n",
    "# mae = mean_absolute_error(target_test2,train_pred2)\n",
    "# print(\"------Results for Random Forest-----\")\n",
    "# print(\"1.....Mean Squared Error: \",mse)\n",
    "# print(\"2.....R score:  \",r)\n",
    "# print(\"3.....Mean Absolute Error: \",mae)\n",
    "\n",
    "print(\"Accuracy Score Random Forest: \",accuracy_score(all_target_testc, all_predc, normalize = True))\n",
    "r = r2_score(all_target_testc, all_predc)\n",
    "mae = mean_absolute_error(all_target_testc,all_predc)\n",
    "print(\"------Results for Random Forest-----\")\n",
    "print(\"1.....R score:  \",r)\n",
    "print(\"2.....Mean Absolute Error: \",mae)\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (mae / all_target_testc)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('......Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "\n",
    "print('scaled-----')\n",
    "\n",
    "n_estimators = [1, 2, 4, 8, 16, 32,42, 64, 100, 200,300,400,470,1100,1150, 1300,1500]\n",
    "\n",
    "for estimator in n_estimators:\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(data_train1)\n",
    "    clf = RandomForestClassifier(n_estimators=estimator, random_state=100)\n",
    "    clf.fit(X_train_scaled,target_train1)\n",
    "    X_test_scaled = scaler.transform(data_test1)\n",
    "    pred1=clf.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(data_train2)\n",
    "    clf = RandomForestClassifier(n_estimators=estimator, random_state=100)\n",
    "    clf.fit(X_train_scaled,target_train2)\n",
    "    X_test_scaled = scaler.transform(data_test2)\n",
    "    pred2=clf.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "    all_pred = np.concatenate((pred1, pred2))\n",
    "    all_target_test = np.concatenate((target_test1, target_test2)) \n",
    "\n",
    "    print(\"Accuracy Score Random Forest: \",accuracy_score(all_target_test, all_pred, normalize = True))\n",
    "    r = r2_score(all_target_test, all_pred)\n",
    "    mae = mean_absolute_error(all_target_test,all_pred)\n",
    "    print(\"------Results for Random Forest-----\")\n",
    "#     print(\"1.....R score:  \",r)\n",
    "#     print(\"2.....Mean Absolute Error: \",mae)\n",
    "\n",
    "#     # Calculate mean absolute percentage error (MAPE)\n",
    "#     mape = 100 * (mae / all_target_test)\n",
    "#     # Calculate and display accuracy\n",
    "#     accuracy = 100 - np.mean(mape)\n",
    "#     print('......Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(all_target_test, all_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
